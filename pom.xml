<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
                             http://maven.apache.org/xsd/maven-4.0.0.xsd">

    <modelVersion>4.0.0</modelVersion>

    <!-- Spring Boot parent for the whole multi-module project -->
    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>3.5.7</version>
        <relativePath/>
    </parent>

    <groupId>com.aicodementor</groupId>
    <artifactId>ai-code-mentor-parent</artifactId>
    <version>1.0.0</version>
    <packaging>pom</packaging>

    <name>AI Code Mentor Parent</name>
    <description>AI-powered programming learning platform</description>

    <properties>
        <java.version>25</java.version>
        <maven.compiler.source>25</maven.compiler.source>
        <maven.compiler.target>25</maven.compiler.target>

        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>

        <!-- Node / npm -->
        <node.version>v18.17.1</node.version>
        <npm.version>9.6.7</npm.version>
        
        <server.executable>server.exe</server.executable>
        <skip.llama.download>false</skip.llama.download>
        <skip.llama.model>false</skip.llama.model>
        <skip.llama>false</skip.llama>
        <skip.llama.server.windows>false</skip.llama.server.windows>
        <skip.llama.server.unix>true</skip.llama.server.unix>
        <skip.llama.unix.download>true</skip.llama.unix.download>
        <skip.backend.windows>false</skip.backend.windows>
        <skip.backend.unix>true</skip.backend.unix>
        <skip.npm.install>false</skip.npm.install>
        <skip.frontend>false</skip.frontend>
        <skip.backend>false</skip.backend>
    </properties>

    <!-- OS-specific profiles -->
    <profiles>
        <!-- Windows profile -->
        <profile>
            <id>windows</id>
            <activation>
                <os>
                    <family>windows</family>
                </os>
            </activation>
            <properties>
                <server.executable>server.exe</server.executable>
                <skip.llama.server.windows>false</skip.llama.server.windows>
                <skip.llama.server.unix>true</skip.llama.server.unix>
                <skip.llama.unix.download>true</skip.llama.unix.download>
                <skip.backend.windows>false</skip.backend.windows>
                <skip.backend.unix>true</skip.backend.unix>
            </properties>
        </profile>

        <!-- Unix/Linux profile -->
        <profile>
            <id>unix</id>
            <activation>
                <os>
                    <family>unix</family>
                </os>
            </activation>
            <properties>
                <server.executable>./server</server.executable>
                <skip.llama.server.windows>true</skip.llama.server.windows>
                <skip.llama.server.unix>false</skip.llama.server.unix>
                <skip.llama.unix.download>false</skip.llama.unix.download>
                <skip.backend.windows>true</skip.backend.windows>
                <skip.backend.unix>false</skip.backend.unix>
            </properties>
        </profile>
    </profiles>

    <!-- Backend module (folder name must be 'backend') -->
    <modules>
        <module>backend</module>
    </modules>

    <repositories>
        <repository>
            <id>central</id>
            <name>Maven Central Repository</name>
            <url>https://repo1.maven.org/maven2</url>
            <releases>
                <enabled>true</enabled>
            </releases>
            <snapshots>
                <enabled>false</enabled>
            </snapshots>
        </repository>
        <repository>
            <id>spring-milestones</id>
            <name>Spring Milestones</name>
            <url>https://repo.spring.io/milestone</url>
            <releases>
                <enabled>true</enabled>
            </releases>
            <snapshots>
                <enabled>false</enabled>
            </snapshots>
        </repository>
    </repositories>

    <pluginRepositories>
        <pluginRepository>
            <id>central</id>
            <name>Maven Plugin Repository</name>
            <url>https://repo1.maven.org/maven2</url>
            <releases>
                <enabled>true</enabled>
            </releases>
            <snapshots>
                <enabled>false</enabled>
            </snapshots>
        </pluginRepository>
        <pluginRepository>
            <id>spring-milestones</id>
            <name>Spring Milestones</name>
            <url>https://repo.spring.io/milestone</url>
            <releases>
                <enabled>true</enabled>
            </releases>
            <snapshots>
                <enabled>false</enabled>
            </snapshots>
        </pluginRepository>
    </pluginRepositories>

    <build>
        <plugins>

            <!-- Download llama.cpp using Antrun plugin -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-antrun-plugin</artifactId>
                <version>3.2.0</version>
                <inherited>false</inherited>
                <executions>

                    <!-- Download llama.cpp (Windows) -->
                    <execution>
                        <id>download-llama-windows</id>
                        <phase>validate</phase>
                        <goals>
                            <goal>run</goal>
                        </goals>
                        <configuration>
                            <target>
                                <!-- Check if Windows -->
                                <condition property="isWindows">
                                    <os family="windows"/>
                                </condition>
                                <!-- Only proceed on Windows -->
                                <antcall target="download-llama-windows-main" if="isWindows"/>
                                <echo message="Skipping Windows download on non-Windows system" unless="isWindows"/>
                            </target>
                            <target name="download-llama-windows-main">
                                <available file="${project.basedir}/llama-cpp/server.exe"
                                           property="llama.exists"/>
                                <antcall target="download-llama-windows-internal"/>
                                <antcall target="skip-download"/>
                            </target>
                            <target name="skip-download" if="llama.exists">
                                <echo message="[OK] llama.cpp found"/>
                            </target>
                            <target name="download-llama-windows-internal" unless="llama.exists">
                                <echo message="[INFO] llama.cpp not found, downloading CUDA version for Windows..."/>
                                <mkdir dir="${project.basedir}/llama-cpp"/>
                                <get src="https://github.com/ggml-org/llama.cpp/releases/download/b7307/llama-b7307-bin-win-cuda-12.4-x64.zip"
                                     dest="${project.basedir}/llama-cpp/llama-cpp.zip"
                                     verbose="true"/>
                                <unzip src="${project.basedir}/llama-cpp/llama-cpp.zip"
                                       dest="${project.basedir}/llama-cpp/temp"
                                       overwrite="true"/>
                                <copy todir="${project.basedir}/llama-cpp"
                                      flatten="true"
                                      includeemptydirs="false">
                                    <fileset dir="${project.basedir}/llama-cpp/temp">
                                        <include name="**/server.exe"/>
                                        <include name="**/*.dll"/>
                                    </fileset>
                                </copy>
                                <delete dir="${project.basedir}/llama-cpp/temp"/>
                                <delete file="${project.basedir}/llama-cpp/llama-cpp.zip"/>
                                <echo message="[OK] llama.cpp installed successfully"/>
                            </target>
                            <skip>${skip.llama.download}</skip>
                        </configuration>
                    </execution>

                   <!-- Download llama.cpp (Unix, e.g. Ubuntu) -->
                <execution>
                    <id>download-llama-unix</id>
                    <phase>validate</phase>
                    <goals>
                        <goal>run</goal>
                    </goals>
                    <configuration>
                        <target>
                            <!-- 只在 Unix 上执行 -->
                            <condition property="isUnix">
                                <os family="unix"/>
                            </condition>

                            <!-- Unix 就跑真正的下载；其他系统直接跳过 -->
                            <antcall target="download-llama-unix-main" if="isUnix"/>
                            <echo message="Skipping Unix download on non-Unix system" unless="isUnix"/>
                        </target>

                        <!-- Unix 主逻辑 -->
                        <target name="download-llama-unix-main">
                            <!-- 如果 server 已经存在就跳过下载 -->
                            <available file="${project.basedir}/llama-cpp/server" property="llama.exists.unix"/>
                            <antcall target="skip-download-unix" if="llama.exists.unix"/>
                            <antcall target="download-llama-unix-internal" unless="llama.exists.unix"/>
                        </target>

                        <target name="skip-download-unix" if="llama.exists.unix">
                            <echo message="[OK] llama.cpp found (Unix)"/>
                        </target>

                        <!-- 真正的 Unix 下载和解压 -->
                        <target name="download-llama-unix-internal" unless="llama.exists.unix">
                            <echo message="[INFO] llama.cpp not found, downloading for Linux x64 with GPU support..."/>
                            <mkdir dir="${project.basedir}/llama-cpp"/>

                            <!-- 下载支持GPU的Linux x64版本 (CUDA版本，如果系统有NVIDIA GPU；否则使用CPU版本但支持OpenCL) -->
                            <!-- 注意：Intel集成显卡不支持CUDA，但可以使用CPU版本或OpenCL加速 -->
                            <get src="https://github.com/ggml-org/llama.cpp/releases/download/b7307/llama-b7307-bin-ubuntu-x64.zip"
                                dest="${project.basedir}/llama-cpp/llama-cpp.zip"
                                verbose="true"/>

                            <!-- 解压 zip（需要系统有 unzip 命令） -->
                            <unzip src="${project.basedir}/llama-cpp/llama-cpp.zip"
                                dest="${project.basedir}/llama-cpp/temp"
                                overwrite="true"/>

                            <!-- 复制 llama-server 和所有共享库 (.so 文件) -->
                            <copy todir="${project.basedir}/llama-cpp" flatten="true" includeemptydirs="false">
                                <fileset dir="${project.basedir}/llama-cpp/temp">
                                    <include name="**/llama-server"/>
                                    <include name="**/*.so"/>
                                    <include name="**/*.so.*"/>
                                </fileset>
                            </copy>

                            <move file="${project.basedir}/llama-cpp/llama-server"
                                tofile="${project.basedir}/llama-cpp/server"
                                failonerror="false"/>

                            <!-- 给可执行权限 -->
                            <chmod file="${project.basedir}/llama-cpp/server" perm="+x"/>
                            
                            <!-- 给共享库可执行权限 -->
                            <chmod type="file" perm="+x">
                                <fileset dir="${project.basedir}/llama-cpp">
                                    <include name="*.so"/>
                                    <include name="*.so.*"/>
                                </fileset>
                            </chmod>

                            <!-- 清理临时文件 -->
                            <delete dir="${project.basedir}/llama-cpp/temp"/>
                            <delete file="${project.basedir}/llama-cpp/llama-cpp.zip"/>

                            <echo message="[OK] llama.cpp installed successfully (Unix)"/>
                        </target>

                        <skip>${skip.llama.download}</skip>
                    </configuration>
                </execution>


                    <!-- Download model -->
                    <execution>
                        <id>download-model</id>
                        <phase>validate</phase>
                        <goals>
                            <goal>run</goal>
                        </goals>
                        <configuration>
                            <target>
                                <available file="${project.basedir}/llama-cpp/models/deepseek-coder-6.7b-instruct.Q2_K.gguf"
                                           property="model.exists"/>
                                <antcall target="skip-model"/>
                                <antcall target="download-model-internal"/>
                            </target>
                            <target name="skip-model" if="model.exists">
                                <echo message="[OK] Model found"/>
                            </target>
                            <target name="download-model-internal" unless="model.exists">
                                <echo message="[INFO] Model not found, downloading... (this may take a while, ~2.5GB)"/>
                                <mkdir dir="${project.basedir}/llama-cpp/models"/>

                                <property name="model.file"
                                          value="${project.basedir}/llama-cpp/models/deepseek-coder-6.7b-instruct.Q2_K.gguf"/>
                                <property name="model.temp"
                                          value="${project.basedir}/llama-cpp/models/deepseek-coder-6.7b-instruct.Q2_K.gguf.tmp"/>

                                <!-- Download to temp file first to avoid file lock issues -->
                                <get src="https://huggingface.co/TheBloke/deepseek-coder-6.7B-instruct-GGUF/resolve/main/deepseek-coder-6.7b-instruct.Q2_K.gguf"
                                     dest="${model.temp}"
                                     verbose="true"/>

                                <!-- Move temp file to final location -->
                                <move file="${model.temp}"
                                      tofile="${model.file}"
                                      failonerror="true"
                                      overwrite="true"/>

                                <echo message="[OK] Model downloaded successfully"/>
                            </target>
                            <skip>${skip.llama.model}</skip>
                        </configuration>
                    </execution>

                </executions>
            </plugin>

            <plugin>
                <groupId>com.github.eirslett</groupId>
                <artifactId>frontend-maven-plugin</artifactId>
                <version>1.15.0</version>
                <inherited>false</inherited>
                <configuration>
                    <!-- package.json is in project root -->
                    <workingDirectory>${project.basedir}</workingDirectory>
                    <!-- Install Node.js and npm to user home directory -->
                    <installDirectory>${user.home}/.aicm-node</installDirectory>
                </configuration>
                <executions>
                    <!-- Install Node + npm -->
                    <execution>
                        <id>install-node-and-npm</id>
                        <phase>validate</phase>
                        <goals>
                            <goal>install-node-and-npm</goal>
                        </goals>
                        <configuration>
                            <nodeVersion>${node.version}</nodeVersion>
                            <npmVersion>${npm.version}</npmVersion>
                            <skip>${skip.npm.install}</skip>
                        </configuration>
                    </execution>

                    <!-- npm install -->
                    <execution>
                        <id>npm-install</id>
                        <phase>validate</phase>
                        <goals>
                            <goal>npm</goal>
                        </goals>
                        <configuration>
                            <arguments>install</arguments>
                            <skip>${skip.npm.install}</skip>
                        </configuration>
                    </execution>
                </executions>
            </plugin>


            <!-- Exec plugin: start frontend + llama server + backend -->
            <plugin>
                <groupId>org.codehaus.mojo</groupId>
                <artifactId>exec-maven-plugin</artifactId>
                <version>3.5.0</version>
                <inherited>false</inherited>
                <executions>

                    <!-- Start frontend dev server: npm run dev -->
                    <execution>
                        <id>start-frontend</id>
                        <phase>process-resources</phase>
                        <goals>
                            <goal>exec</goal>
                        </goals>
                        <configuration>
                            <executable>npm</executable>
                            <workingDirectory>${project.basedir}</workingDirectory>
                            <arguments>
                                <argument>run</argument>
                                <argument>dev</argument>
                            </arguments>
                            <async>true</async>
                            <asyncDestroyOnShutdown>true</asyncDestroyOnShutdown>
                            <skip>${skip.frontend}</skip>
                        </configuration>
                    </execution>

                    <!-- Start llama.cpp server (Windows) -->
                    <execution>
                        <id>start-llama-server-windows</id>
                        <phase>process-resources</phase>
                        <goals>
                            <goal>exec</goal>
                        </goals>
                        <configuration>
                            <executable>cmd</executable>
                            <workingDirectory>${project.basedir}/llama-cpp</workingDirectory>
                            <arguments>
                                <argument>/c</argument>
                                <argument>server.exe</argument>
                                <argument>-m</argument>
                                <argument>models/deepseek-coder-6.7b-instruct.Q2_K.gguf</argument>
                                <argument>-ngl</argument>
                                <argument>35</argument>
                                <argument>-c</argument>
                                <argument>4096</argument>
                                <argument>-t</argument>
                                <argument>4</argument>
                                <argument>-b</argument>
                                <argument>512</argument>
                                <argument>-n</argument>
                                <argument>2048</argument>
                                <argument>--cont-batching</argument>
                                <argument>--port</argument>
                                <argument>11435</argument>
                                <argument>--host</argument>
                                <argument>0.0.0.0</argument>
                            </arguments>
                            <async>true</async>
                            <asyncDestroyOnShutdown>true</asyncDestroyOnShutdown>
                            <skip>${skip.llama.server.windows}</skip>
                        </configuration>
                    </execution>

                    <!-- Start llama.cpp server (Unix/Linux) -->
                    <execution>
                        <id>start-llama-server-unix</id>
                        <phase>process-resources</phase>
                        <goals>
                            <goal>exec</goal>
                        </goals>
                        <configuration>
                            <executable>./server</executable>
                            <workingDirectory>${project.basedir}/llama-cpp</workingDirectory>
                            <environmentVariables>
                                <LD_LIBRARY_PATH>${project.basedir}/llama-cpp:${env.LD_LIBRARY_PATH}</LD_LIBRARY_PATH>
                            </environmentVariables>
                            <arguments>
                                <argument>-m</argument>
                                <argument>models/deepseek-coder-6.7b-instruct.Q2_K.gguf</argument>
                                <argument>-c</argument>
                                <argument>4096</argument>
                                <argument>-t</argument>
                                <argument>4</argument>
                                <argument>-b</argument>
                                <argument>512</argument>
                                <argument>-n</argument>
                                <argument>2048</argument>
                                <argument>--cont-batching</argument>
                                <argument>--port</argument>
                                <argument>11435</argument>
                                <argument>--host</argument>
                                <argument>0.0.0.0</argument>
                                <!-- GPU加速：-ngl 20 表示前20层使用GPU（如果支持OpenCL），其余层使用CPU -->
                                <argument>-ngl</argument>
                                <argument>20</argument>
                            </arguments>
                            <async>true</async>
                            <asyncDestroyOnShutdown>true</asyncDestroyOnShutdown>
                            <skip>${skip.llama.server.unix}</skip>
                        </configuration>
                    </execution>

                    <!-- Run Spring Boot backend module (Windows) -->
                    <execution>
                        <id>run-backend-windows</id>
                        <phase>process-classes</phase>
                        <goals>
                            <goal>exec</goal>
                        </goals>
                        <configuration>
                            <executable>mvn</executable>
                            <workingDirectory>${project.basedir}/backend</workingDirectory>
                            <arguments>
                                <argument>spring-boot:run</argument>
                            </arguments>
                            <async>true</async>
                            <asyncDestroyOnShutdown>true</asyncDestroyOnShutdown>
                            <skip>${skip.backend.windows}</skip>
                        </configuration>
                    </execution>

                    <!-- Run Spring Boot backend module (Unix/Linux) -->
                    <execution>
                        <id>run-backend-unix</id>
                        <phase>process-classes</phase>
                        <goals>
                            <goal>exec</goal>
                        </goals>
                        <configuration>
                            <executable>mvn</executable>
                            <workingDirectory>${project.basedir}/backend</workingDirectory>
                            <arguments>
                                <argument>spring-boot:run</argument>
                            </arguments>
                            <async>true</async>
                            <asyncDestroyOnShutdown>true</asyncDestroyOnShutdown>
                            <skip>${skip.backend.unix}</skip>
                        </configuration>
                    </execution>

                </executions>
            </plugin>

        </plugins>
    </build>

</project>
