# ğŸš€ AICodeMentor

Plateforme d'apprentissage de programmation pilotÃ©e par l'IA - GÃ©nÃ©ration et Ã©valuation automatiques d'exercices de programmation avec LLM local

## âœ¨ CaractÃ©ristiques principales

### ğŸ‘¨â€ğŸ« FonctionnalitÃ©s pour enseignants
- ğŸ¤– **GÃ©nÃ©ration d'exercices par IA** - DÃ©crivez en langage naturel, l'IA gÃ©nÃ¨re automatiquement un exercice complet
- âœï¸ **Ã‰diteur visuel** - Ã‰diteur Monaco pour modifier le code et les tests
- ğŸ“Š **Gestion des Ã©tudiants** - Visualiser les soumissions et statistiques

### ğŸ‘¨â€ğŸ“ FonctionnalitÃ©s pour Ã©tudiants
- ğŸ’» **Ã‰dition de code en ligne** - Ã‰diteur Monaco avec support multi-langages
- âœ… **Tests automatiques** - ExÃ©cution de tests unitaires en temps rÃ©el
- ğŸ’¡ **Indices intelligents** - Indices personnalisÃ©s gÃ©nÃ©rÃ©s par IA

## ğŸ› ï¸ Stack technologique

### Frontend
- Vue.js 3 + Vite
- Bootstrap 5
- Monaco Editor
- Vue Router 4
- Axios

### Backend
- Spring Boot 3.2.0
- Spring Data JPA
- H2 Database (en mÃ©moire)
- LangChain4j
- Maven

### IA
- llama.cpp (exÃ©cution locale)
- ModÃ¨le deepseek-coder-6.7b
- Compatible API LocalAI

## ğŸ“¦ DÃ©marrage rapide

### PrÃ©requis

- **Java 25** (å¿…é¡»å®‰è£…å¹¶è®¾ç½® JAVA_HOME ç¯å¢ƒå˜é‡)
- Node.js 18+
- Maven 3.6+

#### é…ç½® Java 25

1. **ä¸‹è½½å¹¶å®‰è£… Java 25 JDK**
   - ä» Oracle æˆ– OpenJDK å®˜ç½‘ä¸‹è½½ Java 25
   - å®‰è£…åˆ°ç³»ç»Ÿï¼ˆä¾‹å¦‚ï¼š`C:\Program Files\Java\jdk-25` æˆ– `/usr/lib/jvm/java-25`ï¼‰

2. **è®¾ç½® JAVA_HOME ç¯å¢ƒå˜é‡**
   
   **Windows:**
   ```powershell
   # ä¸´æ—¶è®¾ç½®ï¼ˆå½“å‰ä¼šè¯ï¼‰
   $env:JAVA_HOME = "C:\Program Files\Java\jdk-25"
   
   # æ°¸ä¹…è®¾ç½®ï¼ˆç³»ç»Ÿç¯å¢ƒå˜é‡ï¼‰
   # 1. æ‰“å¼€"ç³»ç»Ÿå±æ€§" -> "é«˜çº§" -> "ç¯å¢ƒå˜é‡"
   # 2. æ–°å»ºç³»ç»Ÿå˜é‡ï¼šJAVA_HOME = C:\Program Files\Java\jdk-25
   # 3. ç¼–è¾‘ Path å˜é‡ï¼Œæ·»åŠ ï¼š%JAVA_HOME%\bin
   ```
   
   **Linux/Mac:**
   ```bash
   # ä¸´æ—¶è®¾ç½®ï¼ˆå½“å‰ä¼šè¯ï¼‰
   export JAVA_HOME=/usr/lib/jvm/java-25
   export PATH=$JAVA_HOME/bin:$PATH
   
   # æ°¸ä¹…è®¾ç½®ï¼ˆæ·»åŠ åˆ° ~/.bashrc æˆ– ~/.zshrcï¼‰
   echo 'export JAVA_HOME=/usr/lib/jvm/java-25' >> ~/.bashrc
   echo 'export PATH=$JAVA_HOME/bin:$PATH' >> ~/.bashrc
   source ~/.bashrc
   ```

3. **éªŒè¯å®‰è£…**
   ```bash
   java -version  # åº”è¯¥æ˜¾ç¤º java version "25"
   javac -version # åº”è¯¥æ˜¾ç¤º javac 25
   echo $JAVA_HOME # åº”è¯¥æ˜¾ç¤º Java 25 çš„å®‰è£…è·¯å¾„
   ```

### 1ï¸âƒ£ Installer llama.cpp et le modÃ¨le

```bash
mvn process-classes
```

Cela tÃ©lÃ©chargera environ 4 Go du modÃ¨le CodeLlama, ce qui prendra 10-20 minutes.

### 2ï¸âƒ£ Lancer les services

**é€‰é¡¹ A: ä½¿ç”¨è„šæœ¬ä¸€é”®å¯åŠ¨ï¼ˆCPU æ¨¡å¼ï¼‰**
```bash
# Windows
.\start-all.bat

# Linux/Mac
./start-all.sh
```

**é€‰é¡¹ B: æ‰‹åŠ¨å¯åŠ¨ï¼ˆæ¨è GPU ç”¨æˆ·ï¼‰**

1. **å¯åŠ¨ llama.cpp (å¯é€‰ï¼Œå¦‚æœä½¿ç”¨ GPU è¯·å…ˆæ‰‹åŠ¨å¯åŠ¨):**
   ```bash
   # è§ä¸‹æ–¹ "AccÃ©lÃ©ration GPU" éƒ¨åˆ†è·å–è¯¦ç»†å‘½ä»¤
   cd llama-cpp
   .\server.exe -m models\deepseek-coder-6.7b-instruct.Q2_K.gguf -ngl 35 -c 4096 --port 11435
   ```

2. **å¯åŠ¨åç«¯:**
   ```bash
   cd backend
   # ç¡®ä¿ JAVA_HOME æŒ‡å‘ Java 25
   mvn spring-boot:run
   ```

3. **å¯åŠ¨å‰ç«¯:**
   ```bash
   npm run dev
   ```

ç­‰å¾…æœåŠ¡å¯åŠ¨åï¼Œè®¿é—®ï¼š**http://localhost:3000**

## ğŸ“‹ Ports des services

| Service | Port | Description |
|---------|------|-------------|
| Frontend | 3000 | Serveur de dÃ©veloppement Vue.js |
| Backend | 8080 | API Spring Boot |
| IA | 11435 | Serveur llama.cpp |

> **ğŸ’¡ Note:** é»˜è®¤æƒ…å†µä¸‹ï¼Œ`start-all.bat` ä¼šå¯åŠ¨ CPU æ¨¡å¼çš„ llama.cppã€‚å¦‚æœæ‚¨æœ‰ NVIDIA GPU å¹¶æƒ³ä½¿ç”¨ GPU åŠ é€Ÿï¼Œè¯·å…ˆæ‰‹åŠ¨å¯åŠ¨ GPU ç‰ˆæœ¬çš„ llama-serverï¼ˆè§ä¸‹æ–¹"AccÃ©lÃ©ration GPU"éƒ¨åˆ†ï¼‰ï¼Œç„¶åå†å¯åŠ¨åç«¯å’Œå‰ç«¯ã€‚

## ğŸ“š Structure du projet

```
uni-preset-vue-vite/
â”œâ”€â”€ backend/                    # Backend Spring Boot
â”‚   â”œâ”€â”€ src/main/java/
â”‚   â”‚   â””â”€â”€ com/aicodementor/
â”‚   â”‚       â”œâ”€â”€ controller/     # ContrÃ´leurs REST API
â”‚   â”‚       â”œâ”€â”€ service/        # Logique mÃ©tier et service LLM
â”‚   â”‚       â”œâ”€â”€ entity/         # EntitÃ©s JPA
â”‚   â”‚       â”œâ”€â”€ repository/     # Couche d'accÃ¨s aux donnÃ©es
â”‚   â”‚       â””â”€â”€ dto/            # Objets de transfert de donnÃ©es
â”‚   â””â”€â”€ pom.xml
â”œâ”€â”€ src/                        # Frontend Vue.js
â”‚   â”œâ”€â”€ views/                  # Composants de page
â”‚   â”œâ”€â”€ components/             # Composants rÃ©utilisables
â”‚   â”œâ”€â”€ router/                 # Configuration de routage
â”‚   â”œâ”€â”€ services/               # Services API
â”‚   â””â”€â”€ assets/                 # Ressources statiques
â”œâ”€â”€ llama-cpp/                  # Binaires et modÃ¨les llama.cpp
â”‚   â”œâ”€â”€ server.exe
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ codellama-7b-instruct.Q4_K_M.gguf
â”œâ”€â”€ start-all.bat              # Script de dÃ©marrage en un clic
â”œâ”€â”€ stop-all.bat               # Script d'arrÃªt en un clic
â”œâ”€â”€ install-llamacpp.bat       # Script d'installation LLM
â””â”€â”€ package.json

```

## âš¡ Performances

### Mode CPU (actuel)
- GÃ©nÃ©ration d'exercices : 30 secondes - 2 minutes
- GÃ©nÃ©ration d'indices : 10-30 secondes
- Vitesse : ~5.68 tokens/seconde

### AccÃ©lÃ©ration GPU (optionnel)
Si vous avez une carte graphique NVIDIA, tÃ©lÃ©chargez la version CUDA de llama.cpp pour une accÃ©lÃ©ration 10-50x !

#### æ‰‹åŠ¨å¯åŠ¨ llama.cpp avec GPU

**Windows (PowerShell æˆ– CMD):**
```powershell
# è¿›å…¥ llama-cpp ç›®å½•
cd llama-cpp

# å¯åŠ¨ server.exe ä½¿ç”¨ GPU (CUDA)
# ç¡®ä¿ä½¿ç”¨æ”¯æŒ CUDA çš„ç‰ˆæœ¬ (ggml-cuda.dll å¿…é¡»å­˜åœ¨)
.\server.exe `
  -m models\deepseek-coder-6.7b-instruct.Q2_K.gguf `
  -ngl 35 `
  -c 4096 `
  -t 4 `
  -b 512 `
  -n 2048 `
  --port 11435 `
  --host 0.0.0.0 `
  --cont-batching

# æˆ–è€…ä½¿ç”¨æ›´ç®€å•çš„å‘½ä»¤ï¼ˆæ¨èï¼‰
.\server.exe -m models\deepseek-coder-6.7b-instruct.Q2_K.gguf -ngl 35 -c 4096 --port 11435
```

**Linux/Mac:**
```bash
# è¿›å…¥ llama-cpp ç›®å½•
cd llama-cpp

# å¯åŠ¨ server ä½¿ç”¨ GPU (CUDA)
./server \
  -m models/deepseek-coder-6.7b-instruct.Q2_K.gguf \
  -ngl 35 \
  -c 4096 \
  -t 4 \
  -b 512 \
  -n 2048 \
  --port 11435 \
  --host 0.0.0.0 \
  --cont-batching

# æˆ–è€…ä½¿ç”¨æ›´ç®€å•çš„å‘½ä»¤ï¼ˆæ¨èï¼‰
./server -m models/deepseek-coder-6.7b-instruct.Q2_K.gguf -ngl 35 -c 4096 --port 11435
```

**å‚æ•°è¯´æ˜:**
- `-m`: æ¨¡å‹æ–‡ä»¶è·¯å¾„
- `-ngl`: åœ¨ GPU ä¸Šè¿è¡Œçš„å±‚æ•°ï¼ˆ0 = ä»… CPUï¼Œ20-35 = å¤§éƒ¨åˆ†åœ¨ GPUï¼Œå–å†³äº GPU å†…å­˜ï¼‰
- `-c`: ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆ4096 æˆ– 8192ï¼‰
- `-t`: CPU çº¿ç¨‹æ•°ï¼ˆå¦‚æœéƒ¨åˆ†å±‚åœ¨ CPU ä¸Šè¿è¡Œï¼‰
- `-b`: æ‰¹å¤„ç†å¤§å°ï¼ˆ512 æˆ–æ›´é«˜ï¼Œå–å†³äº GPU å†…å­˜ï¼‰
- `-n`: æœ€å¤§ç”Ÿæˆ token æ•°ï¼ˆ2048 æˆ–æ›´é«˜ï¼‰
- `--port`: æœåŠ¡ç«¯å£ï¼ˆé»˜è®¤ 11435ï¼Œ**æ³¨æ„ï¼šä½¿ç”¨ `--port` è€Œä¸æ˜¯ `-p`**ï¼‰
- `--host`: ç»‘å®šåœ°å€ï¼ˆ0.0.0.0 å…è®¸å¤–éƒ¨è®¿é—®ï¼‰
- `--cont-batching`: å¯ç”¨è¿ç»­æ‰¹å¤„ç†ï¼ˆæé«˜æ€§èƒ½ï¼‰

**âš ï¸ é‡è¦æç¤º:**
- ä½¿ç”¨ `server.exe` (Windows) æˆ– `server` (Linux/Mac)ï¼Œè€Œä¸æ˜¯ `llama-server.exe`
- ç«¯å£å‚æ•°æ˜¯ `--port`ï¼Œ**ä¸æ˜¯** `-p`
- GPU å±‚æ•°å‚æ•°æ˜¯ `-ngl`ï¼Œ**ä¸æ˜¯** `--gpu-layers`

**æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨:**
```bash
# Windows
.\server.exe --help | findstr ngl

# Linux/Mac
./server --help | grep ngl
```

**æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‚æ•°:**
```bash
# Windows
.\server.exe --help

# Linux/Mac
./server --help
```

**éªŒè¯ GPU åŠ é€Ÿ:**
å¯åŠ¨åï¼Œæ£€æŸ¥æ—¥å¿—ä¸­æ˜¯å¦æœ‰ç±»ä¼¼ä¿¡æ¯ï¼š
```
llama_model_load_internal: using CUDA for GPU acceleration
llama_model_load_internal: n_gpu_layers = 35
```

**æ³¨æ„äº‹é¡¹:**
- ç¡®ä¿å®‰è£…äº† NVIDIA CUDA Toolkit å’Œ cuDNN
- ç¡®ä¿ `ggml-cuda.dll` (Windows) æˆ– `libggml-cuda.so` (Linux) å­˜åœ¨äº llama-cpp ç›®å½•
- å¦‚æœ GPU å†…å­˜ä¸è¶³ï¼Œå‡å°‘ `--gpu-layers` å‚æ•°
- å¦‚æœé‡åˆ°é”™è¯¯ï¼Œå°è¯• `--gpu-layers 0` ä½¿ç”¨çº¯ CPU æ¨¡å¼

## ğŸ¯ Flux d'utilisation

### Workflow enseignant

1. Se connecter au systÃ¨me
2. Cliquer sur "CrÃ©er un exercice"
3. DÃ©crire les exigences de l'exercice en langage naturel
4. L'IA gÃ©nÃ¨re un exercice complet (Ã©noncÃ©, code, tests, solution)
5. Examiner et publier

### Workflow Ã©tudiant

1. Se connecter au systÃ¨me
2. Parcourir les exercices disponibles
3. Ã‰crire du code dans l'Ã©diteur Monaco
4. Soumettre et exÃ©cuter automatiquement les tests
5. Consulter les rÃ©sultats et indices gÃ©nÃ©rÃ©s par IA

## ğŸ› DÃ©pannage

### Port occupÃ©
```bash
.\stop-all.bat
# Attendre 5 secondes
.\start-all.bat
```

### Frontend vide
Appuyer sur F12 pour voir les erreurs dans la console du navigateur

### Ã‰chec de gÃ©nÃ©ration IA
VÃ©rifier que llama.cpp est en cours d'exÃ©cution :
```bash
# Windows PowerShell
Invoke-WebRequest -Uri http://localhost:11435/health

# Linux/Mac
curl http://localhost:11435/health
```

### GPU ä¸å·¥ä½œ
1. **æ£€æŸ¥ CUDA å®‰è£…:**
   ```bash
   # Windows
   nvidia-smi
   
   # Linux
   nvidia-smi
   ```

2. **æ£€æŸ¥ llama.cpp GPU æ”¯æŒ:**
   - ç¡®ä¿ä¸‹è½½äº†æ”¯æŒ CUDA çš„ç‰ˆæœ¬
   - æ£€æŸ¥ `llama-cpp` ç›®å½•ä¸­æ˜¯å¦æœ‰ `ggml-cuda.dll` (Windows) æˆ–ç›¸å…³ CUDA åº“

3. **é™çº§åˆ° CPU æ¨¡å¼:**
   å¦‚æœ GPU æœ‰é—®é¢˜ï¼Œå¯ä»¥æ‰‹åŠ¨å¯åŠ¨çº¯ CPU æ¨¡å¼ï¼š
   ```bash
   # Windows
   .\server.exe -m models\deepseek-coder-6.7b-instruct.Q2_K.gguf -ngl 0 -c 4096 --port 11435
   
   # Linux/Mac
   ./server -m models/deepseek-coder-6.7b-instruct.Q2_K.gguf -ngl 0 -c 4096 --port 11435
   ```

## ğŸ“„ Licence

MIT License

## ğŸ¤ Contribution

Les Issues et Pull Requests sont les bienvenues !

---

**Commencer : ExÃ©cutez `.\start-all.bat` puis accÃ©dez Ã  http://localhost:3000** ğŸ‰
