# üöÄ AICodeMentor

Plateforme d'apprentissage de programmation pilot√©e par l'IA - G√©n√©ration et √©valuation automatiques d'exercices de programmation avec LLM local.

## üìã Table des mati√®res

1. [Pr√©sentation](#-pr√©sentation)
2. [Fonctionnalit√©s](#-fonctionnalit√©s)
3. [Pr√©requis](#-pr√©requis)
4. [Installation](#-installation)
5. [D√©marrage](#-d√©marrage)
6. [Configuration](#-configuration)
7. [Utilisation](#-utilisation)
8. [D√©pannage](#-d√©pannage)

---

## üéØ Pr√©sentation

AICodeMentor est une plateforme √©ducative qui utilise l'intelligence artificielle pour g√©n√©rer automatiquement des exercices de programmation et fournir des retours personnalis√©s aux √©tudiants. Le syst√®me utilise un mod√®le de langage local (llama.cpp) pour fonctionner enti√®rement hors ligne, garantissant la confidentialit√© des donn√©es.

### Objectifs

- **Pour les enseignants** : Cr√©er rapidement des exercices de programmation de qualit√©
- **Pour les √©tudiants** : Pratiquer la programmation avec des retours instantan√©s et des indices intelligents
- **Pour les institutions** : Solution open-source, locale et respectueuse de la vie priv√©e

---

## ‚ú® Fonctionnalit√©s

### üë®‚Äçüè´ Pour les enseignants

- **ü§ñ G√©n√©ration d'exercices par IA**
  - Description en langage naturel ‚Üí Exercice complet g√©n√©r√© automatiquement
  - Inclut : √©nonc√©, code de d√©part, tests unitaires, solution, exemples

- **‚úèÔ∏è √âditeur visuel**
  - √âditeur Monaco (m√™me moteur que VS Code)
  - Modification du code g√©n√©r√© et des tests
  - Pr√©visualisation en temps r√©el

- **üìä Gestion des √©tudiants**
  - Visualisation des soumissions
  - Statistiques de performance
  - Suivi des progr√®s

- **üìù Publication d'exercices**
  - Contr√¥le de la visibilit√© (publi√©/brouillon)
  - Organisation par th√®me et difficult√©
  - Recherche et filtrage

### üë®‚Äçüéì Pour les √©tudiants

- **üíª √âdition de code en ligne**
  - √âditeur Monaco avec coloration syntaxique
  - Support multi-langages (actuellement Java)
  - Auto-compl√©tion et validation

- **‚úÖ Tests automatiques**
  - Ex√©cution de tests unitaires en temps r√©el
  - Retour imm√©diat sur les r√©sultats
  - Affichage des erreurs de compilation et d'ex√©cution

- **üí° Indices intelligents**
  - Indices personnalis√©s g√©n√©r√©s par IA
  - Analyse du code de l'√©tudiant
  - Suggestions contextuelles sans r√©v√©ler la solution

---

## üìã Pr√©requis

### Logiciels requis

1. **Java 25 JDK** (obligatoire)
   - T√©l√©charger depuis [Oracle](https://www.oracle.com/java/technologies/downloads/) ou [OpenJDK](https://jdk.java.net/)
   - **Important** : Configurer la variable d'environnement `JAVA_HOME`

2. **Node.js 18+** (pour le frontend)
   - T√©l√©charger depuis [nodejs.org](https://nodejs.org/)

3. **Maven 3.6+** (pour le backend)
   - T√©l√©charger depuis [maven.apache.org](https://maven.apache.org/download.cgi)

### Configuration de Java 25

#### Windows

```powershell
# 1. D√©finir JAVA_HOME (session actuelle)
$env:JAVA_HOME = "C:\Program Files\Java\jdk-25"

# 2. Ajouter au PATH
$env:PATH = "$env:JAVA_HOME\bin;$env:PATH"

# 3. V√©rifier l'installation
java -version
javac -version
```

**Configuration permanente** :
1. Ouvrir "Param√®tres syst√®me" ‚Üí "Variables d'environnement"
2. Cr√©er une variable syst√®me : `JAVA_HOME = C:\Program Files\Java\jdk-25`
3. Modifier `Path` : ajouter `%JAVA_HOME%\bin`

#### Linux / macOS

```bash
# 1. D√©finir JAVA_HOME (session actuelle)
export JAVA_HOME=/usr/lib/jvm/java-25
export PATH=$JAVA_HOME/bin:$PATH

# 2. V√©rifier l'installation
java -version
javac -version
```

**Configuration permanente** :
```bash
# Ajouter √† ~/.bashrc ou ~/.zshrc
echo 'export JAVA_HOME=/usr/lib/jvm/java-25' >> ~/.bashrc
echo 'export PATH=$JAVA_HOME/bin:$PATH' >> ~/.bashrc
source ~/.bashrc
```

---

## üì¶ Installation

### √âtape 1 : Cloner le projet

```bash
git clone <repository-url>
cd ai-code-mentor-main
```

### √âtape 2 : Installer les d√©pendances frontend

```bash
npm install
```

### √âtape 3 : T√©l√©charger le mod√®le IA

Le mod√®le sera t√©l√©charg√© automatiquement lors de la premi√®re compilation :

```bash
cd backend
mvn process-classes
```

**Note** : Le t√©l√©chargement prend environ 10-20 minutes (mod√®le de ~2.5GB).

Le mod√®le sera plac√© dans : `llama-cpp/models/deepseek-coder-6.7b-instruct.Q2_K.gguf`

**Note** : Vous pouvez utiliser d'autres versions quantis√©es (Q4_K_M, Q5_K_M, etc.) en t√©l√©chargeant manuellement le mod√®le et en mettant √† jour la configuration dans `application.yml`.

---

## üöÄ D√©marrage

Le syst√®me n√©cessite 3 services qui doivent √™tre d√©marr√©s dans l'ordre :

### 1. D√©marrer llama.cpp (serveur LLM)

**Windows** :
```powershell
cd llama-cpp
.\llama-server.exe -m models\deepseek-coder-6.7b-instruct.Q2_K.gguf -ngl 0 -c 4096 --port 11435
```

**Linux / macOS** :
```bash
cd llama-cpp
./server -m models/deepseek-coder-6.7b-instruct.Q2_K.gguf -ngl 0 -c 4096 --port 11435
```

**Note** : Si vous utilisez un autre mod√®le (Q4_K_M, Q5_K_M, etc.), remplacez `Q2_K` par le nom de votre mod√®le dans la commande ci-dessus.

**V√©rification** : Attendre le message "HTTP server listening" dans la console.

**Note** : Gardez cette fen√™tre ouverte. Le serveur doit rester actif.

### 2. D√©marrer le backend (Spring Boot)

Ouvrir un **nouveau terminal** :

```bash
cd backend
mvn spring-boot:run
```

**V√©rification** : Attendre le message "Started AiCodeMentorApplication" et v√©rifier : http://localhost:8080/api/exercises

**Note** : Gardez cette fen√™tre ouverte.

### 3. D√©marrer le frontend (Vue.js)

Ouvrir un **nouveau terminal** :

```bash
npm run dev
```

**V√©rification** : Le terminal affichera l'URL (g√©n√©ralement http://localhost:5173)

### Acc√®s √† l'application

Une fois les 3 services d√©marr√©s, acc√©der √† : **http://localhost:5173** (ou l'URL affich√©e par Vite)

---

## ‚öôÔ∏è Configuration

### Configuration du backend

Fichier : `backend/src/main/resources/application.yml`

```yaml
# Port du serveur
server:
  port: 8080

# Base de donn√©es H2
spring:
  datasource:
    url: jdbc:h2:file:./data/testdb
    username: sa
    password: password

# Configuration LLM
llm:
  provider: llamacpp
  llamacpp:
    base-url: http://localhost:11435
    model: deepseek-coder-6.7b-instruct.Q2_K  # Ou Q4_K_M, Q5_K_M, etc.
    timeout: 300  # Augmenter √† 300 pour Q4/Q5
```

### Configuration du frontend

Le proxy est configur√© dans `vite.config.js` pour rediriger `/api` vers `http://localhost:8080` en d√©veloppement.

---

## ‚ö° Acc√©l√©ration GPU (optionnel)

### NVIDIA GPU (CUDA)

**D√©marrage avec GPU** :
```bash
cd llama-cpp
.\llama-server.exe -m models\deepseek-coder-6.7b-instruct.Q2_K.gguf -ngl 35 -c 4096 --port 11435
```

**Note** : Remplacez `Q2_K` par votre mod√®le si vous utilisez une autre version.

**Param√®tres recommand√©s** :
- `-ngl 35` : 35 couches sur GPU (ajuster selon la m√©moire GPU)
- `-c 4096` : Taille du contexte
- `-t 4` : Threads CPU (pour les couches restantes)

**V√©rification** :
```bash
nvidia-smi  # V√©rifier l'utilisation GPU
```

### Mode CPU (par d√©faut)

Si vous n'avez pas de GPU ou rencontrez des probl√®mes, utilisez le mode CPU :
```bash
-ngl 0  # Toutes les couches sur CPU
```

---

## üéØ Utilisation

### Workflow enseignant

1. **Cr√©er un exercice**
   - Se connecter avec un compte enseignant (teacher@demo.com / demo123)
   - Acc√©der √† "Cr√©er"
   - D√©crire l'exercice en langage naturel (ex: "√âcrire une fonction qui inverse une cha√Æne")
   - L'IA g√©n√®re automatiquement : √©nonc√©, code de d√©part, tests, solution, exemples

2. **Modifier et personnaliser**
   - Utiliser l'√©diteur pour ajuster le code
   - Modifier les tests si n√©cessaire
   - Ajouter des indices ou des explications

3. **Publier**
   - Activer le statut "Publi√©"
   - L'exercice devient visible pour les √©tudiants

### Workflow √©tudiant

1. **Parcourir les exercices**
   - Se connecter avec un compte √©tudiant (student@demo.com / demo123)
   - Consulter la liste des exercices publi√©s
   - Filtrer par th√®me ou difficult√©
   - S√©lectionner un exercice

2. **R√©soudre l'exercice**
   - Lire l'√©nonc√© et les exemples
   - √âcrire le code dans l'√©diteur
   - Cliquer sur "Ex√©cuter les tests"

3. **Obtenir de l'aide**
   - Si les tests √©chouent, consulter les erreurs
   - Cliquer sur "Obtenir un Indice" (g√©n√©r√© automatiquement par IA)
   - It√©rer jusqu'√† r√©solution

### Comptes de d√©monstration

- **Enseignant 1** : `teacher@demo.com` / `demo123`
- **Enseignant 2** : `teacher2@demo.com` / `demo123`
- **√âtudiant** : `student@demo.com` / `demo123`

---

## üêõ D√©pannage

### Probl√®me : Port d√©j√† utilis√©

**Port 8080 (backend)** :
```bash
# Windows
netstat -ano | findstr :8080
taskkill /PID <PID> /F

# Linux/Mac
lsof -ti:8080 | xargs kill -9
```

**Port 11435 (llama.cpp)** :
```bash
# Windows
netstat -ano | findstr :11435
taskkill /PID <PID> /F

# Linux/Mac
lsof -ti:11435 | xargs kill -9
```

### Probl√®me : Frontend vide ou erreurs

1. Ouvrir la console du navigateur (F12)
2. V√©rifier les erreurs dans l'onglet "Console"
3. V√©rifier que le backend est accessible : http://localhost:8080/api/exercises

### Probl√®me : G√©n√©ration IA √©choue

**V√©rifier llama.cpp** :
```bash
# Windows PowerShell
Invoke-WebRequest -Uri http://localhost:11435/health

# Linux/Mac
curl http://localhost:11435/health
```

**Si non accessible** :
1. V√©rifier que llama.cpp est d√©marr√©
2. V√©rifier le port (11435)
3. V√©rifier les logs pour les erreurs
4. V√©rifier que le mod√®le est pr√©sent dans `llama-cpp/models/`

### Probl√®me : Erreur de compilation Java

**V√©rifier Java 25** :
```bash
java -version  # Doit afficher version 25
echo $JAVA_HOME  # Doit pointer vers Java 25 (Linux/Mac)
echo %JAVA_HOME%  # Doit pointer vers Java 25 (Windows)
```

**Si probl√®me** :
1. R√©installer Java 25
2. Reconfigurer `JAVA_HOME`
3. Red√©marrer le terminal

### Probl√®me : Mod√®le non trouv√©

Si le mod√®le n'est pas t√©l√©charg√© automatiquement :

1. T√©l√©charger manuellement depuis HuggingFace :
   - URL : `https://huggingface.co/TheBloke/deepseek-coder-6.7B-instruct-GGUF`
   - Fichiers disponibles : Q2_K (~2.5GB), Q4_K_M (~4GB), Q5_K_M (~5GB), Q8_0 (~7GB)
   - Placer dans : `llama-cpp/models/`

2. Mettre √† jour la configuration dans `application.yml` :
   ```yaml
   llm:
     llamacpp:
       model: deepseek-coder-6.7b-instruct.Q4_K_M  # Nom du mod√®le t√©l√©charg√©
   ```

3. Utiliser le nom du mod√®le dans la commande de d√©marrage de llama.cpp

---

## üèóÔ∏è Architecture technique

### Frontend
- **Vue.js 3.4+** - Framework frontend
- **Vite 5.2+** - Build tool et serveur de d√©veloppement
- **Bootstrap 5.3+** - Framework CSS
- **Monaco Editor** - √âditeur de code

### Backend
- **Spring Boot 3.5+** - Framework backend
- **Spring Data JPA** - Acc√®s aux donn√©es
- **H2 Database** - Base de donn√©es (fichier)
- **JUnit 5** - Ex√©cution de tests
- **Maven** - Gestion des d√©pendances

### Intelligence artificielle
- **llama.cpp** - Serveur d'inf√©rence local
- **deepseek-coder-6.7b-instruct** - Mod√®le de langage sp√©cialis√© en code
- **Format** : GGUF (quantis√© Q2_K par d√©faut, ~2.5GB)

### Ports des services

| Service | Port | URL |
|---------|------|-----|
| Frontend | 5173 | http://localhost:5173 |
| Backend API | 8080 | http://localhost:8080 |
| llama.cpp | 11435 | http://localhost:11435 |

---

## üìö Structure du projet

```
ai-code-mentor-main/
‚îú‚îÄ‚îÄ backend/                    # Backend Spring Boot
‚îÇ   ‚îú‚îÄ‚îÄ src/main/java/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ com/aicodementor/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ controller/     # Contr√¥leurs REST API
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ service/        # Logique m√©tier
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ entity/         # Entit√©s JPA
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ repository/    # Acc√®s aux donn√©es
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ config/         # Configuration
‚îÇ   ‚îî‚îÄ‚îÄ pom.xml
‚îú‚îÄ‚îÄ src/                        # Frontend Vue.js
‚îÇ   ‚îú‚îÄ‚îÄ views/                  # Pages
‚îÇ   ‚îú‚îÄ‚îÄ components/             # Composants r√©utilisables
‚îÇ   ‚îú‚îÄ‚îÄ services/               # Services API
‚îÇ   ‚îî‚îÄ‚îÄ router/                 # Routage
‚îú‚îÄ‚îÄ llama-cpp/                  # Binaires et mod√®les
‚îÇ   ‚îú‚îÄ‚îÄ llama-server.exe/server # Serveur llama.cpp
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îî‚îÄ‚îÄ deepseek-coder-6.7b-instruct.Q2_K.gguf
‚îî‚îÄ‚îÄ package.json
```

---

## üíª D√©veloppement

### Compilation

**Backend** :
```bash
cd backend
mvn clean compile
```

**Frontend** :
```bash
npm run build
```

### Base de donn√©es

**Console H2** : http://localhost:8080/h2-console

**Connexion** :
- JDBC URL: `jdbc:h2:file:./data/testdb`
- Username: `sa`
- Password: `password`

### Logs

Les logs sont configur√©s dans `application.yml` :
```yaml
logging:
  level:
    com.aicodementor: DEBUG
```

---

## ü§ù Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† :
- Ouvrir une Issue pour signaler un bug
- Proposer une Pull Request
- Am√©liorer la documentation

---

## üìÑ Licence

Ce projet est open-source.
